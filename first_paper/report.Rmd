---
title: "First Paper - Stroke Analysis"
author: "Ordinary Leading Students"
date: "6/02/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
header-includes:
- \usepackage{subfig}
- \usepackage{bbm}
urlcolor: blue
---

```{r, include=FALSE, message=FALSE}

# Load libraries
library(ggplot2)        # Basic plots
library(gridExtra)      # Multiple plot same figure
library(dplyr)          # Play w/ dataframes
library(broom)          # tidy()
library(GoodmanKruskal) # nice Corr Matrix
library(Hmisc)          # compute rcorr between quantitative vars
library(corrplot)       # plot nice corr matrix
library(caret)          # trainControl (train tree predictor)
library(pROC)
#library(performanceEstimation)

###################################################################
# SMOTE stuff
library(ROCR)

remotes::install_github("dongyuanwu/RSBID")
library(RSBID) #cmd above to install it
###################################################################

# Set the current directory as working directory and load custom functions.
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
source("../utils/plots.R")
source("../utils/data_analysis.R")

```


\begin{abstract}
In the following paper we will present a statistical analysis of stroke related data. Our goal will be to predict whether a patient is likely to get stroke based on multiple input parameters like gender, age, various diseases, and smoking status.
Each row in the data provides relevant information about the patient.

After some data cleaning, we plotted some interesting variables of our dataset to get some insight on
the content and structure of our data.
We then proceeded to the analysis. 


\end{abstract}

\section{Data}

We will now load and present our dataset (that can be retrieved at the following [link](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset)). \

\begin{enumerate}
\item id: unique identifier
\item gender: [Male, Female, Other]
\item age: age of the patient
\item hypertension = $\begin{cases} 1 \quad \text{if the patient has hypertension}\\  0 \quad \text{otherwise} \end{cases}$
\item heart\_disease = $\begin{cases} 1 \quad \text{if the patient has a heart disease}\\  0 \quad \text{otherwise} \end{cases}$
\item ever\_married = [Yes, No]
\item work\_type = [children, Govt\_jov, Never\_worked, Private, Self-employed]
\item Residence\_type = [Rural, Urban]
\item avg\_glucose\_level: average glucose level in blood
\item bmi: body mass index
\item smoking\_status = [formerly smoked, never smoked, smokes, Unknown\footnote{The information is unavailable for this patient}]
\item stroke = $\begin{cases} 1 \quad \text{if the patient had a stroke}\\  0 \quad \text{otherwise} \end{cases}$
\end{enumerate}
\newpage
```{r}
# Load and summarize dataset
dataset = read.csv("stroke.csv")
str(dataset)
```


\section{Data Cleaning}

We convert our variables in the right data type and we show the updated summary.\

```{r, warning=FALSE}
# Remove id column (useless)
dataset$id = NULL

# Convert qualitative variables in factors
dataset$gender          = factor(dataset$gender)
dataset$hypertension    = factor(dataset$hypertension, levels = c(0,1), labels = c("No", "Yes"))
dataset$heart_disease   = factor(dataset$heart_disease, levels = c(0,1), labels = c("No", "Yes"))
dataset$ever_married    = factor(dataset$ever_married)
dataset$work_type       = factor(dataset$work_type)
dataset$residence_type  = factor(dataset$Residence_type)
dataset$smoking_status  = factor(dataset$smoking_status)
dataset$stroke          = factor(dataset$stroke, levels = c(0,1), labels = c("No", "Yes"))

# Convert quantitative variable in numeric
dataset$bmi             = as.numeric(dataset$bmi)

# Remove duplicate column (due to uppercase)
dataset$Residence_type  = NULL 

# Show dataset summary
str(dataset)

```
\section{ Data Visualization}

We can use different boxplots to visualize possible relations between our quantitative variables (**age**, **bmi** and **average glucose level**) and stroke status.\

```{r, warning=FALSE, fig.width=10}
grid.arrange(ggplot(dataset, aes(x=stroke ,y=age)) +
              geom_boxplot(fill= "#FDE725FF", alpha= 0.7), 
            ggplot(dataset, aes(x=stroke, y=bmi))+
              geom_boxplot(fill= "#2D708EFF", alpha= 0.7), 
            ggplot(dataset, aes(x=stroke, y=avg_glucose_level))+
              geom_boxplot(fill= "#440154FF", alpha= 0.7), 
            ncol=3)
```

From the plots above we can infer the following:
\begin{itemize}
\item \textbf{Age}: older people are more likely to have a stroke.
\item \textbf{Bmi}: there is no evident relation between stroke and bmi.
\item \textbf{Average glucose level}: the higher the level of glucose, the higher the relation with stroke
\end{itemize}

Below we plot a few matrices to visualize possible relations between our qualitative variables and stroke status.\

```{r, warning=FALSE, fig.width=10, fig.height=7, fig.align='center'}
grid.arrange(factors_plot(tidy(table(dataset %>% dplyr::select(stroke,work_type))), palette='Blues',
                         font_count_size=4, font_normalized_size=5.1, font_percentages_size=2.5,
                         font_categories_size=10), 
            factors_plot(tidy(table(dataset %>% dplyr::select(stroke,smoking_status))), palette='Greens',
                         font_count_size=4, font_normalized_size=5.1, font_percentages_size=2.5,
                         font_categories_size=10),
            factors_plot(tidy(table(dataset %>% dplyr::select(stroke,gender))), palette='Purples',
                         font_count_size=4, font_normalized_size=5.1, font_percentages_size=2.5,
                         font_categories_size=10),
            ncol=3, nrow=1)
```
```{r, warning=FALSE, fig.width=10, fig.height=5, fig.align='center'}
grid.arrange(factors_plot(tidy(table(dataset %>% dplyr::select(stroke,hypertension))), palette='Greens',
                         font_count_size=3.5, font_normalized_size=5, font_percentages_size=2.5,
                         font_categories_size=10), 
            factors_plot(tidy(table(dataset %>% dplyr::select(stroke,heart_disease))), palette='Blues',
                         font_count_size=3.5, font_normalized_size=5, font_percentages_size=2.5,
                         font_categories_size=10), 
            factors_plot(tidy(table(dataset %>% dplyr::select(stroke,residence_type))), palette='Purples',
                         font_count_size=3.5, font_normalized_size=5, font_percentages_size=2.5,
                         font_categories_size=10),
            factors_plot(tidy(table(dataset %>% dplyr::select(stroke,ever_married))), palette='Oranges',
                         font_count_size=3.5, font_normalized_size=5, font_percentages_size=2.5,
                         font_categories_size=10),
            ncol=2, nrow=2)
# \footnote{Further informations on this subject can be found at the following links: \begin{itemize}
#                         \item [Work type](TODO)
#                         \item [Smoking status](https://ourworldindata.org/smoking)
#                         \item [Hypertension](https://www.medscape.com/answers/241381-7614/what-is-the-global-prevalence-of-hypertension-high-blood-pressure)
#                         \item [Heart diseases](https://pubmed.ncbi.nlm.nih.gov/32742886/)
#                        \end{itemize}}
```

From these comparison matrices we can deduce the following:
\begin{itemize}
\item A lot of our subjects work in the private sector (57.24\%) and do not smoke (37\%), we can also notice a high predominance of subjects without hypertension (90.25\%) and heart diseases (94.6\%). These data should not worry us because they do not indicate unbalance in our dataset but just reflects the actual distributon of global population\footnote{Further informations on this subject can be found at the following links: \begin{itemize}
                        \item \href{TODO}{Work type}
                        \item \href{https://ourworldindata.org/smoking}{Smoking status}
                        \item \href{https://www.medscape.com/answers/241381-7614/what-is-the-global-prevalence-of-hypertension-high-blood-pressure}{Hypertension}
                        \item \href{https://pubmed.ncbi.nlm.nih.gov/32742886/}{Heart diseases}
                       \end{itemize}}.
\item We miss a lot of information on the smoking status of our partecipants (30.2\% unknown), this could evolve in a modellization problem if we do not address it properly.
\item A lot of our subjects did not suffered a stroke (95.1\%), this could cause us some problem but we will address it using the SMOTE technique.
\end{itemize}

\newpage

And we can finally show **qualitative** and **quantitative** correlations.

```{r, warning=FALSE, fig.align='center'}
qualitative_vars = c('gender', 'hypertension', 'heart_disease', 'ever_married',
                      'work_type', 'smoking_status', 'residence_type')
plot(GKtauDataframe(dataset %>% dplyr::select(all_of(qualitative_vars))))
```
```{r, fig.height=4, fig.width=5, fig.align='center'}

quantitative_vars = c('age', 'avg_glucose_level', 'bmi')
corr <- rcorr(as.matrix(dataset %>% dplyr::select(all_of(quantitative_vars))))
corrplot(corr$r, type = "upper", tl.col = "black", tl.srt = 45)
``` 

From the results above we can notice that we don't have multicollinearity in our instrumental variables. The only two correlations that stand out are the one between work_type and ever_married and the one between age and bmi, as expected. \textbf{TODO explain better?}

\section{Data Manipulation}

In order to facilitate our work, we removed the level **other** from the variable gender and the level **never_worked** from the variable work_type, since they are not relevant for the analysis.\

\textbf{SHOULD WE REMOVE UNKNNOWN SMOKING STATUS???}
```{r}
# Remove rows
dataset = dataset %>%
          filter(gender != 'Other') %>%
          filter(work_type != 'Never_worked')

# Remove related levels
dataset = droplevels(dataset)
```

\subsection{NA Values}

We now look for NA values in our dataset:

```{r}
# Check for NA in all columns
for (col_name in colnames(dataset)){
   if (anyNA(dataset[[col_name]]))
    print(paste(col_name, '-> ', sum(is.na(dataset[[col_name]])), ' NA'))
}
```

We notice that we have 201 NA just in the **bmi** variable. In order to avoid losing these rows, given that we already have a small dataset, we will predict their values using a fitted tree.\

```{r, warning=FALSE, message=FALSE}

# Define test set as set with missing data
missing_index <- which(is.na(dataset$bmi))

test_set <- dataset[missing_index,]
train_set <- dataset[-c(missing_index),]

# Fit the tree
tree = caret::train(bmi ~ ., 
                    data=train_set, 
                    method="rpart", 
                    trControl = trainControl(method = "cv"))

# Replace missing data with predicted data
bmi_pred <- predict(tree, newdata = test_set)
dataset[missing_index, 'bmi'] <- bmi_pred

# clean global environment
rm(test_set, train_set, tree, bmi_pred, missing_index)
```


\section{Statistical Model}

We will now begin splitting our dataset in train/test.\

```{r}
# Set a fixed seed in order to have reproducible results
set.seed(36)

# 60-40 split
split_train_test <- createDataPartition(y = dataset$stroke, p=0.6, list = F)
train <- dataset[split_train_test,]
test <-  dataset[-split_train_test,]

print(table(train$stroke))
print(table(test$stroke))
```

\subsection{SMOTE algorithm}

Since our dataset has a problem of under sampling, we will use the \href{https://arxiv.org/pdf/1106.1813.pdf}{SMOTE} algorithm to create synthetic new data.\
```{r, warning=FALSE, message=FALSE}
# New smote algorithm
train_smoted <- SMOTE_NC(train, "stroke", k = 2)

# Now we have a balanced dataset
table(train_smoted$stroke)
```
Notice that the SMOTE algorithm was only applied to the train set, this is done on pourpose to avoid the evaluation of our final model on synthetic data.\

\subsection{Logistic Regression}

We perform a logistic regression using  \textbf{stroke} as the dependent variable and the rest of the data as independent variables in order to see if we find some significant relationships.

```{r}
Logit <- glm(stroke~., data=train, family = binomial(link = "logit"))
summary(Logit)
```
The only significant variables are **age**, **hypertension** and **average glucose level**, 
which have a positive impact on the increase of the logit probability.
**ADDRESS UNKNOWN SMOKING STATUS PROBLEM**

\subsection{Confusion Matrix and Statistics}

We will now evaluate our model over the test set.
```{r}
# THIS? Big chuck of code for no results?
lr_prob1 <- predict(Logit, newdata = test, type = "response")

lr_preds_test <- c(0,0,0,0,0,0,0,0,0,0,0)

i<-1
for (thresh in seq(0.25,0.75,0.05)){
  lr_pred <- ifelse(lr_prob1 > thresh,1,0)
  #cm <- matrix(1:4, nrow=2)
  cm <- table(
    Predicted = factor(lr_pred, levels = c(0,1)),
    Actual = factor(test$stroke, levels = c(0,1))
  )[2:1, 2:1]
  lr_preds_test[i] <- F_meas(cm) # f1 score
  i<-i+1
}
names(lr_preds_test) <- seq(0.25,0.75,0.05)
lr_preds_test
lr_pred <- as.numeric(ifelse(lr_prob1 > 0.75,"1","0"))
tb <- table(
        Predicted = factor(lr_pred, levels = c(0,1)),
        Actual = factor(test$stroke, levels = c(0,1))
      )[2:1, 2:1]
tb
(tb[1:1,1:1] + tb[2:2, 2:2])/(tb[1:1,2:2] + tb[2:2, 1:1] + tb[1:1,1:1] + tb[2:2, 2:2]) #Accuracy
F_meas(tb) # F1
recall(tb)  # Recall
precision(tb) # Precision
```
```{r}
library(caret)
lr_prob1 <- predict(Logit, newdata = test, type = "response")
lr_pred1 <- ifelse(lr_prob1 > 0.1,"Yes","No")
confusionMatrix(
  as.factor(lr_pred1),
  as.factor(test$stroke),
  positive = "Yes" 
)
```
```{r, warning=FALSE, message=FALSE}
test_roc <- roc(as.numeric(test$stroke)~lr_prob1 , plot = TRUE, print.auc = TRUE,percent=TRUE, ci=TRUE)
```
```{r, warning=FALSE, message=FALSE}
print(test_roc$auc)
```
As we can see from the outputs above, our model does not perform very well. 
With a threshold over 0.1 it predicts correctly just 62.6\% of the positive results (in our case, people who suffered a stroke), at the expense of 16\% of false positives.
Because this is such a sensitive subject, we would definitely prefer having more false positives than false negatives, but lowering our threshold, say to 0.05, increase our sensitivity to 81.8\%, but it lowers the specificity to 71.7\% (from 83.7\%).

The problem of this underperforming model could be addressed in two different ways:
\begin{enumerate}
\item We could speak with the stakeholders to understand how to best tune our threshold. In a real-world scenario this would translate into a discussion regarding the number of people we can take care of, even if they are false positives, in order to avoid rejecting people who actually need our help.
\item We could try and increase the accuracy of our model, and we will do that below using a Boosting technique.
\end{enumerate}

\section{Boosting}

Boosting is a technique used that convert weak learners to strong ones, and it works by iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier.
Notice that, when thery are added to the model, all the learners are weighted with relation to their prediction accuracy.
```{r}
#WHY?
# for (i in 1:11) {
#   levels(train_smoted[,i]) <- make.names(c(levels(train_smoted[,i])))
# }
# for (i in 1:11) {
#   levels(test[,i]) <- make.names(c(levels(test[,i])))
# }

# Tuning parameter grid
gbmGrid <- expand.grid(nIter=c(16,34,102))


trctrl <- trainControl(method = "cv"
                       , number = 3
                      # , repeats = 5
                      # , search = "random"
                       , classProbs = T
                       , summaryFunction = twoClassSummary
                       )

logit_fit <- train(stroke ~., data = train_smoted, method = "LogitBoost", trControl=trctrl,
                   tuneGrid=gbmGrid, 
                   metric= "ROC"
                   )
logit_fit
```
We can notice in the output above that boosting our model slightly improves its accuracy (AUC from 85\% to 86.6\%) when 102 iterations are computed. 
```{r}
plot(logit_fit)
```
\section{Conclusion}
\textbf{TODO}