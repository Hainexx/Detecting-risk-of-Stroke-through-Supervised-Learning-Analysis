---
title: "Second Paper"
author: "Ordinary Leading Students"
date: "6/02/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
header-includes:
- \usepackage{subfig}
- \usepackage{bbm}
urlcolor: blue
---

```{r, include=FALSE, message=FALSE}
# Loading libraries
library(ggplot2)
library(grid)
library(gridExtra)
library(cluster)
library(readr)
library(corrplot) 
library(factoextra)
library(png)
```


\begin{abstract}
In this paper we are going to perform a cluster analysis on a dataset used for a 
market basket analysis. Our goal is to find unknown subgroups in the dataset.

\end{abstract}

\section{Data}
We will now load and present our dataset (that can be retrieved at the following [link](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)). 

```{r, message=FALSE}
# Set the working directory and load the dataset
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

malldt<- read_csv("Mall_Customers.csv")

# Summary of the dataset without the id column
summary(malldt[-1])
```

\section{Data Cleaning}

We removed the **CustomerID** variable and converted the **Gender** one in factor.

```{r}
malldt$CustomerID <- NULL
malldt$Gender <- as.factor(malldt$Gender)
```

\newpage

\section{Data Visualization}


```{r}
# Gender Boxplots

par(mfrow=c(1,2))
boxplot(`Annual Income (k$)`~Gender,data=malldt, col="#2D708EFF")
boxplot(`Spending Score (1-100)`~Gender,data=malldt, col="darkgoldenrod2")

```

As we can see there are no meaningful differences between the genders group.

\newpage

```{r}
# Age Boxplots

# Create age groups
labs <- c(paste(seq(15, 70, by = 20), seq(15 + 20 - 1, 80, by = 20), 
                sep = "-"))

malldt$AgeGroup <- cut(malldt$Age, breaks = c(seq(15, 70, by = 20), Inf), 
                       labels = labs, right = FALSE)

par(mfrow=c(1,2))
boxplot(`Annual Income (k$)`~AgeGroup,data=malldt, col="#FF99CC")
boxplot(`Spending Score (1-100)`~AgeGroup,data=malldt, col="#33CC66")
```

The only interesting information we can retrieve from the plots is that the youngest (age between 15 and 34) have an **higher spending score**, but not other significant information stand out.

```{r, include=FALSE}
# Remove the age groups
malldt$AgeGroup <- NULL
```

\newpage

\section{Dendograms}

Now we start finding subgroups using clustering techniques.

First we used the **Gower's distance** to build dendrograms with different methods. \
This distance can be used to measure how different two records are and it 
is always a number between 0 (identical) and 1 (maximally dissimilar). It
is computed as the average of partial dissimilarities across individuals.

```{r}
# Gower distance works for mixed variables
malldt.dist<-daisy(malldt,metric="gower")

# Dendrograms
malldt.hc.com<-hclust(malldt.dist,method="complete") 
plot(malldt.hc.com, main="Complete Method") 
rect.hclust(malldt.hc.com,k=3,border=c("red","green","blue")) 

malldt.hc.sin<-hclust(malldt.dist,method="single") 
plot(malldt.hc.sin, main="Single Method") 
rect.hclust(malldt.hc.sin,k=3,border=c("red","green","blue")) 

malldt.hc.ave<-hclust(malldt.dist,method="average") 
plot(malldt.hc.ave, main="Average Method") 
rect.hclust(malldt.hc.ave,k=3,border=c("red","green","blue")) 

malldt.hc.cen<-hclust(malldt.dist,method="centroid") 
plot(malldt.hc.cen, main="Centroid Method") 
rect.hclust(malldt.hc.cen,k=3,border=c("red","green","blue")) 

malldt.hc.ward<-hclust(malldt.dist,method="ward.D2") 
plot(malldt.hc.ward, main="Ward Method") 
rect.hclust(malldt.hc.ward,k=8,border=c("red","green","blue"))

```

\subsection{Ward Method}

The **Ward Method** is clearly the best and from it we can see that there are 8 groups.

```{r}
# Allocate obs into 8 groups
malldt.groups.ward<-cutree(malldt.hc.ward,k=8) 
malldt.groups.ward

# Number of observations in each group
table(malldt.groups.ward)
```


Below a table that shows for each group the mean value of each variable. 


```{r}
clusterdata.mean<-function(data,groups){
  aggregate(data,list(groups),function(x)mean(as.numeric(x)))
}

clusterdata.mean(malldt,malldt.groups.ward)
```

From it we can see that some groups are not so different with each other.

Group 4 and group 5 differs significantly only for the **gender**, whereas the other variables are more or less in the same range. \
Group 6 and group 8, on the other hand, differs significantly only for the **spending score**. \

We have obtained a good result, but it can still be improved, since there are clear overlappings between the groups we got.


\section{K-Means algorithm}

Now we apply the K-means algorithm.

First we have to remove the categorical variable **Gender** from the code because
the algorithm only supports numerical variables.

```{r}
malldtstd<-scale(malldt[,-1]) 
```

\subsection{Finding K}

```{r}
set.seed(42)
k.max<-15 

wss<-sapply(1:k.max,function(k){kmeans(malldtstd,k,nstart=50,iter.max=15)$tot.withinss})

plot(1:k.max,wss,type="b",pch=19,xlab="Number of groups",ylab="Within Deviation",col="blue") 
```

Let's try with k=4, k=5 and k=6 and evaluate their performances with the
**silhuotte method**.

\subsection{Silhoutte}

This refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified.

The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from 1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

```{r, echo = T, results = 'hide'}
kmeans4<-kmeans(malldt[,-1],4)
kmeans5<-kmeans(malldt[,-1],5)
kmeans6<-kmeans(malldt[,-1],6)

# Evaluation of the clustering composition, images will be shown later
#ris4<- eclust(malldt[,-1],"kmeans",k=4)
#ris5<- eclust(malldt[,-1],"kmeans",k=5)
#ris6<- eclust(malldt[,-1],"kmeans",k=6)
```
```{r, include=FALSE}
ris4<- eclust(malldt[,-1],"kmeans",k=4)
ris5<- eclust(malldt[,-1],"kmeans",k=5)
ris6<- eclust(malldt[,-1],"kmeans",k=6)
```
```{r, echo = T, results = 'hide'}
# Dimensions and average of group's silhouette
avg_s_4 <- fviz_silhouette(ris4) + labs(title= "K = 4",
                                        subtitle= " Avg Silhoutte width: 0.39")
avg_s_5 <- fviz_silhouette(ris5) + labs(title= "K = 5",
                                        subtitle= " Avg Silhoutte width: 0.38")
avg_s_6 <- fviz_silhouette(ris6)+ labs(title= "K = 6",
                                        subtitle= " Avg Silhoutte width: 0.34")
```


```{r, echo=FALSE}
groups_4 <- readPNG("ris4.png")
groups_5 <- readPNG("ris5.png")
groups_6 <- readPNG("ris6.png")

grid.arrange(rasterGrob(groups_4), avg_s_4,
             rasterGrob(groups_5), avg_s_5,
             rasterGrob(groups_6), avg_s_6,
             ncol=2)

```


From the plots on the left we can see if the groups are well defined or if they overlap.
It is clear that the higher the value of k, the higher the problems of overlapping.

Moreover, as we can see from the plots on the right, k=4 has also the best average silhouette value (0.39). From these plots it is also evident that k=4 has no observation with a negative silhouette value.

We decided to check this last point, searching for every exact observations with negative silhouette value.


```{r}
# Silhouette measure of each observation
sil4<-ris4$silinfo$widths 
sil5<-ris5$silinfo$widths
sil6<-ris6$silinfo$widths

# Position of observation of silhouette < 0
neg_sil_index4<-which(sil4[,'sil_width']<0)
neg_sil_index5<-which(sil5[,'sil_width']<0)
neg_sil_index6<-which(sil6[,'sil_width']<0)

# Observations with silhouette < 0
sil4[neg_sil_index4,]
sil5[neg_sil_index5,]
sil6[neg_sil_index6,]

```



This again confirms that with k=4 we obtain zero observations with negative silhouette value, whereas with k=5 and k=6 we have respectevely 7 and 10 observation of this kind.






